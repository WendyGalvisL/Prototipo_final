{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creación dataset ficticio"
      ],
      "metadata": {
        "id": "VnaID0t-B_TK"
      },
      "id": "VnaID0t-B_TK"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def generar_datos_ficticios_completos(n=200, seed=42):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Catálogos de valores\n",
        "    # ---------------------------\n",
        "    direccion_vals = [\n",
        "        \"Kr 14 # 21 105\", \"kra 46A # 110_64\", \"Calle 10 4 48 capincito\",\n",
        "        \"Carrera 3 6 124\", \"Cea 3 6 20\", \"Kra 46A # 110_60\"\n",
        "    ]\n",
        "\n",
        "    municipio_nombre_vals = [\n",
        "        \"Barranquilla\",\"Medellín\",\"Bucaramanga\",\"Bogotá, D.C.\",\"Cúcuta\",\"Cali\",\n",
        "        \"Cartagena\",\"Sincelejo\",\"Buenavista\",\"San Marcos\",\"Itagüí\",\"Soacha\",\n",
        "        \"Popayán\",\"Pasto\",\"Mocoa\",\"Santa Marta\",\"Villavicencio\",\"Chía\",\"Neiva\"\n",
        "    ]\n",
        "\n",
        "    nombre_municipio_vals = [\n",
        "        \"BARRANQUILLA\",\"MEDELLÍN\",\"BUCARAMANGA\",\"BOGOTÁ, D.C.\",\n",
        "        \"SAN JOSÉ DE CÚCUTA\",\"SANTIAGO DE CALI\",\"CARTAGENA DE INDIAS\",\"SINCELEJO\",\n",
        "        \"POPAYÁN\",\"PASTO\",\"MOCOA\",\"SANTA MARTA\",\"VILLAVICENCIO\",\"CHÍA\",\"NEIVA\"\n",
        "    ]\n",
        "\n",
        "    nombre_depto_vals = [\n",
        "        \"ATLÁNTICO\",\"ANTIOQUIA\",\"SANTANDER\",\"BOGOTÁ, D.C.\",\"NORTE DE SANTANDER\",\n",
        "        \"VALLE DEL CAUCA\",\"BOLÍVAR\",\"SUCRE\",\"CAUCA\",\"NARIÑO\",\"PUTUMAYO\",\n",
        "        \"MAGDALENA\",\"META\",\"HUILA\"\n",
        "    ]\n",
        "\n",
        "    dane5_vals = [8001, 5001, 68001, 11001, 54001, 76001, 13001, 70001, 25175]\n",
        "\n",
        "    # ---------------------------\n",
        "    # DataFrame\n",
        "    # ---------------------------\n",
        "    df = pd.DataFrame({\n",
        "        \"AÑO\": np.random.choice(range(2018, 2024), size=n),\n",
        "        \"GPSLAT\": np.random.choice([np.nan] + list(np.random.uniform(4, 11, n)), size=n),\n",
        "        \"GPSLONG\": np.random.choice([np.nan] + list(np.random.uniform(-76, -74, n)), size=n),\n",
        "        \"SECTOR\": np.random.randint(1, 100, size=n),\n",
        "        \"SECCION\": np.random.randint(1, 10, size=n),\n",
        "        \"MANZANA\": np.random.randint(1, 50, size=n),\n",
        "        \"DIRECCION_FILTRO\": np.random.choice(direccion_vals, size=n),\n",
        "        \"DANE5\": np.random.choice(dane5_vals, size=n),\n",
        "        \"PB1\": np.random.choice([1,2], size=n),\n",
        "        \"PERSONAS\": np.random.choice([1,2,3,4,5,6,7,8,9,10], size=n),\n",
        "        \"EDAD\": np.random.randint(12, 80, size=n),\n",
        "        \"REDAD\": np.random.choice([1,2,3,4,5,6,7], size=n),\n",
        "        \"GENERO\": np.random.choice([1,2], size=n),\n",
        "        \"ESTRATO\": np.random.choice([0,1,2,3,4,5,6], size=n),\n",
        "        \"P9\": np.random.choice([np.nan] + list(np.random.rand(10)), size=n),\n",
        "        \"P10\": np.random.choice([\"88\",\"01\",\"010203\",\"112123\"], size=n),\n",
        "        \"P34\": np.random.choice([np.nan] + list(np.random.rand(10)), size=n),\n",
        "        \"P56\": np.random.choice([np.nan] + list(np.random.rand(10)), size=n),\n",
        "        \"P57\": np.random.choice([np.nan] + list(np.random.rand(10)), size=n),\n",
        "        \"P64\": np.random.choice([np.nan] + list(np.random.rand(10)), size=n),\n",
        "        \"NIVEL_PIRAMIDE\": np.random.choice([0,1,2,3], size=n),\n",
        "        \"POBLACION_5_16\": np.random.uniform(100, 5000, size=n),\n",
        "        \"Estudiantes_5_16\": np.random.uniform(50, 3000, size=n),\n",
        "        \"PROP_EDUC_5_16_MEN\": np.random.uniform(0, 1, size=n),\n",
        "        \"HOGARES_INTERNET\": np.random.randint(10, 80000, size=n),\n",
        "        \"POBLACIÓN_ICFES\": np.random.randint(50, 90000, size=n),\n",
        "        \"TASA_INTERNET_ICFES\": np.random.uniform(0, 1, size=n),\n",
        "        \"MUNICIPIO_NOMBRE\": np.random.choice(municipio_nombre_vals, size=n),\n",
        "        \"Nombre Municipio\": np.random.choice(nombre_municipio_vals, size=n),\n",
        "        \"Nombre Departamento\": np.random.choice(nombre_depto_vals, size=n),\n",
        "        \"Servicios_Telecomunicaciones_No\": np.random.choice([0,1], size=n),\n",
        "        \"Servicios_Telecomunicaciones_Si\": np.random.choice([0,1], size=n),\n",
        "        \"Dispositivos_hogar_No\": np.random.choice([True, False], size=n),\n",
        "        \"conexion_hogar_si\": np.random.choice([True, False], size=n),\n",
        "        \"interrupciones_si\": np.random.choice([True, False], size=n),\n",
        "        \"frec_uso_si\": True,\n",
        "        \"dens_int\": np.random.uniform(0, 1, size=n)\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# Ejemplo de uso\n",
        "df_ficticio = generar_datos_ficticios_completos(200)"
      ],
      "metadata": {
        "id": "0oEmG5JwCCDW"
      },
      "id": "0oEmG5JwCCDW",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación modelo"
      ],
      "metadata": {
        "id": "pP8NmwBDq8h1"
      },
      "id": "pP8NmwBDq8h1"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "04d52328",
      "metadata": {
        "id": "04d52328"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 0) Imports y utilidades\n",
        "# ============================================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score\n",
        "\n",
        "# -------- Helpers de preparación --------\n",
        "FEATURES = [\n",
        "    \"Servicios_Telecomunicaciones_No\",\n",
        "    \"Servicios_Telecomunicaciones_Si\",\n",
        "    \"Dispositivos_hogar_No\",\n",
        "    \"conexion_hogar_si\",\n",
        "    \"interrupciones_si\",\n",
        "    \"frec_uso_si\",\n",
        "    \"dens_int\",\n",
        "]\n",
        "\n",
        "# Variables SOLO para perfilar (no entrenan el modelo)\n",
        "SOCIO_GEO = [\n",
        "    \"EDAD\", \"ESTRATO\", \"NIVEL_PIRAMIDE\",\n",
        "    \"MUNICIPIO_NOMBRE\", \"Nombre Departamento\"\n",
        "]\n",
        "\n",
        "def load_data(path=\"data_procesada.csv\"):\n",
        "    df = pd.read_csv(path)\n",
        "    # Asegurar columnas esperadas\n",
        "    missing = [c for c in FEATURES if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Faltan columnas en data_procesada: {missing}\")\n",
        "\n",
        "    # Coerción a numérico/bool\n",
        "    for c in FEATURES:\n",
        "        # Si es binario, mapear True/False/NaN a 1/0\n",
        "        if c != \"dens_int\":\n",
        "            df[c] = df[c].astype(\"float\").astype(\"Int64\")  # por si viene como 0/1/NaN\n",
        "            df[c] = df[c].fillna(0).astype(int)\n",
        "        else:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    # Imputación mínima para dens_int\n",
        "    if df[\"dens_int\"].isna().any():\n",
        "        df[\"dens_int\"] = df[\"dens_int\"].fillna(df[\"dens_int\"].median())\n",
        "\n",
        "    return df\n",
        "\n",
        "def apply_weights(X: pd.DataFrame, weights: dict | None):\n",
        "    \"\"\"Multiplica columnas por pesos; útil para el Experimento 2.\"\"\"\n",
        "    Xw = X.copy()\n",
        "    if weights:\n",
        "        for col, w in weights.items():\n",
        "            if col in Xw.columns:\n",
        "                Xw[col] = Xw[col].astype(float) * float(w)\n",
        "    return Xw\n",
        "\n",
        "def scale_numeric(X: pd.DataFrame, numeric_cols=(\"dens_int\",)):\n",
        "    Xs = X.copy()\n",
        "    scaler = StandardScaler()\n",
        "    Xs[list(numeric_cols)] = scaler.fit_transform(Xs[list(numeric_cols)])\n",
        "    return Xs\n",
        "\n",
        "# -------- Métricas y estabilidad --------\n",
        "def compute_internal_metrics(X: np.ndarray, labels: np.ndarray) -> dict:\n",
        "    # Silhouette requiere al menos 2 clústers y no todos los puntos iguales\n",
        "    sil = silhouette_score(X, labels) if len(np.unique(labels)) > 1 else np.nan\n",
        "    ch  = calinski_harabasz_score(X, labels) if len(np.unique(labels)) > 1 else np.nan\n",
        "    db  = davies_bouldin_score(X, labels) if len(np.unique(labels)) > 1 else np.nan\n",
        "    return {\"silhouette\": sil, \"calinski_harabasz\": ch, \"davies_bouldin\": db}\n",
        "\n",
        "def profile_sharpness(centroids: pd.DataFrame, binary_cols: list[str]) -> float:\n",
        "    \"\"\"Promedio de distancia del centroide a 0.5 (cuanto mayor, más 'nítido').\"\"\"\n",
        "    if not binary_cols:\n",
        "        return np.nan\n",
        "    diffs = []\n",
        "    for col in binary_cols:\n",
        "        if col in centroids.columns:\n",
        "            diffs.append(np.abs(centroids[col] - 0.5))\n",
        "    if not diffs:\n",
        "        return np.nan\n",
        "    return float(pd.concat(diffs, axis=1).mean().mean())\n",
        "\n",
        "def cluster_size_ok(labels: np.ndarray, min_ratio=0.02) -> bool:\n",
        "    n = len(labels)\n",
        "    sizes = pd.Series(labels).value_counts(normalize=True)\n",
        "    return (sizes >= min_ratio).all()\n",
        "\n",
        "def stability_ari(X: np.ndarray, k: int, seeds: list[int], subsample=0.8) -> float:\n",
        "    \"\"\"Promedio de ARI entre múltiples corridas con submuestreo.\"\"\"\n",
        "    label_list = []\n",
        "    rng = np.random.default_rng(42)\n",
        "    n = X.shape[0]\n",
        "    for seed in seeds:\n",
        "        idx = rng.choice(n, size=int(n*subsample), replace=False)\n",
        "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=seed)\n",
        "        labels = kmeans.fit_predict(X[idx])\n",
        "        # Volver a tamaño completo (asignación a vecinos más cercanos)\n",
        "        # Para estabilidad comparamos solo intersecciones\n",
        "        label_list.append((idx, labels))\n",
        "\n",
        "    # Promedio de ARI en intersecciones\n",
        "    if len(label_list) < 2:\n",
        "        return np.nan\n",
        "    aris = []\n",
        "    for i in range(len(label_list)):\n",
        "        for j in range(i+1, len(label_list)):\n",
        "            idx_i, lab_i = label_list[i]\n",
        "            idx_j, lab_j = label_list[j]\n",
        "            common = np.intersect1d(idx_i, idx_j, assume_unique=False)\n",
        "            if len(common) < 10:\n",
        "                continue\n",
        "            # Mapear a posiciones en cada subconjunto\n",
        "            map_i = pd.Series(range(len(idx_i)), index=idx_i)\n",
        "            map_j = pd.Series(range(len(idx_j)), index=idx_j)\n",
        "            li = lab_i[map_i.loc[common].values]\n",
        "            lj = lab_j[map_j.loc[common].values]\n",
        "            aris.append(adjusted_rand_score(li, lj))\n",
        "    return float(np.mean(aris)) if aris else np.nan\n",
        "\n",
        "def geographic_entropy(df: pd.DataFrame, labels: np.ndarray, col_region=\"Nombre Departamento\") -> float:\n",
        "    \"\"\"Entropía normalizada promedio de la distribución regional por clúster.\"\"\"\n",
        "    df_ = df[[col_region]].copy()\n",
        "    df_[\"cluster\"] = labels\n",
        "    entropies = []\n",
        "    for c, g in df_.groupby(\"cluster\"):\n",
        "        p = g[col_region].value_counts(normalize=True)\n",
        "        H = -(p * np.log(p + 1e-12)).sum()\n",
        "        H_max = np.log(len(p)) if len(p) > 1 else 1.0\n",
        "        entropies.append(H / H_max)\n",
        "    return float(np.mean(entropies)) if entropies else np.nan\n",
        "\n",
        "def centroid_table(X: pd.DataFrame, labels: np.ndarray):\n",
        "    \"\"\"Promedio por clúster (binarios → tasas; dens_int → media).\"\"\"\n",
        "    dfc = X.copy()\n",
        "    dfc[\"cluster\"] = labels\n",
        "    cent = dfc.groupby(\"cluster\").mean(numeric_only=True).sort_index()\n",
        "    sizes = dfc[\"cluster\"].value_counts().sort_index().to_frame(name=\"n\")\n",
        "    sizes[\"pct\"] = sizes[\"n\"] / len(X)\n",
        "    return cent, sizes\n",
        "\n",
        "# -------- Bucle de experimentos --------\n",
        "def run_kmeans_grid(df: pd.DataFrame, k_values, weights=None, exp_name=\"exp\"):\n",
        "    # 1) Subconjunto de features\n",
        "    X = df[FEATURES].copy()\n",
        "    # 2) Pesos (opcional para Exp. 2)\n",
        "    X = apply_weights(X, weights)\n",
        "    # 3) Escalar dens_int\n",
        "    X = scale_numeric(X, numeric_cols=(\"dens_int\",))\n",
        "    X_np = X.values\n",
        "\n",
        "    results = []\n",
        "    best = None\n",
        "\n",
        "    for k in k_values:\n",
        "        km = KMeans(n_clusters=k, n_init=50, init=\"k-means++\", random_state=123)\n",
        "        labels = km.fit_predict(X_np)\n",
        "\n",
        "        # Métricas internas\n",
        "        m = compute_internal_metrics(X_np, labels)\n",
        "\n",
        "        # Tamaño mínimo por clúster\n",
        "        ok = cluster_size_ok(labels, min_ratio=0.02)\n",
        "\n",
        "        # Centroides y nitidez de perfil\n",
        "        cent, sizes = centroid_table(X, labels)\n",
        "        ps = profile_sharpness(cent, [c for c in FEATURES if c != \"dens_int\"])\n",
        "\n",
        "        # Estabilidad (rápida): seeds más pequeñas para la malla\n",
        "        ari = stability_ari(X_np, k, seeds=[0,1,2,3,4], subsample=0.8)\n",
        "\n",
        "        row = {\n",
        "            \"exp\": exp_name, \"k\": k, \"silhouette\": m[\"silhouette\"],\n",
        "            \"calinski_harabasz\": m[\"calinski_harabasz\"], \"davies_bouldin\": m[\"davies_bouldin\"],\n",
        "            \"profile_sharpness\": ps, \"stability_ari\": ari,\n",
        "            \"min_size_ok\": ok, \"sizes\": sizes[\"pct\"].round(3).to_dict()\n",
        "        }\n",
        "        results.append(row)\n",
        "\n",
        "        # Actualizar mejor (por Silhouette, luego CH, luego PS y estabilidad)\n",
        "        if ok:\n",
        "            if best is None:\n",
        "                best = (row, labels, cent, sizes)\n",
        "            else:\n",
        "                br = best[0]\n",
        "                better = (\n",
        "                    (row[\"silhouette\"] > br[\"silhouette\"] + 1e-6) or\n",
        "                    (np.isclose(row[\"silhouette\"], br[\"silhouette\"]) and row[\"calinski_harabasz\"] > br[\"calinski_harabasz\"] + 1e-6) or\n",
        "                    (np.isclose(row[\"silhouette\"], br[\"silhouette\"]) and np.isclose(row[\"calinski_harabasz\"], br[\"calinski_harabasz\"]) and row[\"profile_sharpness\"] > br[\"profile_sharpness\"] + 1e-6) or\n",
        "                    (np.isclose(row[\"silhouette\"], br[\"silhouette\"]) and np.isclose(row[\"calinski_harabasz\"], br[\"calinski_harabasz\"]) and np.isclose(row[\"profile_sharpness\"], br[\"profile_sharpness\"]) and row[\"stability_ari\"] > br[\"stability_ari\"] + 1e-6)\n",
        "                )\n",
        "                if better:\n",
        "                    best = (row, labels, cent, sizes)\n",
        "\n",
        "    results_df = pd.DataFrame(results).sort_values([\"silhouette\",\"calinski_harabasz\"], ascending=[False, False])\n",
        "    return results_df, best\n",
        "\n",
        "def interpret_clusters(df: pd.DataFrame, labels: np.ndarray):\n",
        "    \"\"\"Perfiles con variables socio/geo (no utilizadas en el entrenamiento).\"\"\"\n",
        "    out = {}\n",
        "\n",
        "    # Anexar labels\n",
        "    tmp = df.copy()\n",
        "    tmp[\"cluster\"] = labels\n",
        "\n",
        "    # Resumen socio-demográfico\n",
        "    socio_num = tmp.groupby(\"cluster\")[[\"EDAD\", \"ESTRATO\"]].agg([\"mean\",\"median\",\"std\",\"count\"])\n",
        "    # NIVEL_PIRAMIDE distribución\n",
        "    piramide = (tmp\n",
        "                .groupby([\"cluster\", \"NIVEL_PIRAMIDE\"])\n",
        "                .size()\n",
        "                .groupby(level=0)\n",
        "                .apply(lambda s: (s / s.sum()).round(3))\n",
        "                .unstack(fill_value=0))\n",
        "\n",
        "    # Top departamentos y municipios por cluster\n",
        "    depto = (tmp.groupby([\"cluster\",\"Nombre Departamento\"]).size()\n",
        "                .groupby(level=0)\n",
        "                .apply(lambda s: (s / s.sum()).sort_values(ascending=False).head(5))\n",
        "                .to_frame(\"share\"))\n",
        "    mpio = (tmp.groupby([\"cluster\",\"MUNICIPIO_NOMBRE\"]).size()\n",
        "                .groupby(level=0)\n",
        "                .apply(lambda s: (s / s.sum()).sort_values(ascending=False).head(5))\n",
        "                .to_frame(\"share\"))\n",
        "\n",
        "    out[\"socio_num\"] = socio_num\n",
        "    out[\"nivel_piramide\"] = piramide\n",
        "    out[\"top_departamentos\"] = depto\n",
        "    out[\"top_municipios\"] = mpio\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# 1) Cargar datos\n",
        "# ============================================\n",
        "df = load_data(\"data_procesada.csv\")\n",
        "\n",
        "# ============================================\n",
        "# 2) Experimento 1 — Línea base\n",
        "# ============================================\n",
        "k_values_exp1 = [4,5,6,7,8,9,10,12]\n",
        "res1, best1 = run_kmeans_grid(df, k_values_exp1, weights=None, exp_name=\"exp1\")\n",
        "print(\"Resumen EXP1 (top 10):\")\n",
        "print(res1.head(10))\n",
        "\n",
        "# Interpretación del mejor de EXP1\n",
        "row1, labels1, cent1, sizes1 = best1\n",
        "geoH1 = geographic_entropy(df, labels1, col_region=\"Nombre Departamento\")\n",
        "print(\"\\nMejor EXP1:\", row1)\n",
        "print(\"Entropía geográfica (promedio):\", round(geoH1, 3))\n",
        "print(\"\\nCentroides EXP1:\")\n",
        "print(cent1.round(3))\n",
        "print(\"\\nTamaños EXP1:\")\n",
        "print(sizes1.assign(pct = (sizes1[\"pct\"]*100).round(1)))\n",
        "\n",
        "interp1 = interpret_clusters(df, labels1)\n",
        "# Ejemplo de impresión breve:\n",
        "print(\"\\nTop departamentos EXP1:\")\n",
        "print(interp1[\"top_departamentos\"].head(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbxlYUtBC5BJ",
        "outputId": "0538cacf-1216-4973-a189-dea12ad56d13"
      },
      "id": "LbxlYUtBC5BJ",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumen EXP1 (top 10):\n",
            "    exp   k  silhouette  calinski_harabasz  davies_bouldin  profile_sharpness  \\\n",
            "7  exp1  12    0.581389        5727.729515        0.786539           0.469081   \n",
            "6  exp1  10    0.552298        5373.025104        0.797002           0.458214   \n",
            "5  exp1   9    0.526628        5215.895152        0.862672           0.446199   \n",
            "4  exp1   8    0.503722        5078.551223        0.929814           0.455454   \n",
            "3  exp1   7    0.489833        5008.885657        0.978842           0.437764   \n",
            "2  exp1   6    0.473070        4838.416996        0.932473           0.441245   \n",
            "1  exp1   5    0.436763        4937.847432        0.936604           0.423702   \n",
            "0  exp1   4    0.376060        4788.867507        1.031835           0.391998   \n",
            "\n",
            "   stability_ari  min_size_ok  \\\n",
            "7       0.955508         True   \n",
            "6       0.881761         True   \n",
            "5       0.784771         True   \n",
            "4       0.839268         True   \n",
            "3       0.838855         True   \n",
            "2       0.861025         True   \n",
            "1       0.972946         True   \n",
            "0       0.828196         True   \n",
            "\n",
            "                                               sizes  \n",
            "7  {0: 0.055, 1: 0.235, 2: 0.075, 3: 0.092, 4: 0....  \n",
            "6  {0: 0.045, 1: 0.18, 2: 0.125, 3: 0.088, 4: 0.2...  \n",
            "5  {0: 0.087, 1: 0.236, 2: 0.124, 3: 0.097, 4: 0....  \n",
            "4  {0: 0.181, 1: 0.097, 2: 0.056, 3: 0.278, 4: 0....  \n",
            "3  {0: 0.282, 1: 0.107, 2: 0.204, 3: 0.125, 4: 0....  \n",
            "2  {0: 0.048, 1: 0.204, 2: 0.191, 3: 0.284, 4: 0....  \n",
            "1  {0: 0.221, 1: 0.285, 2: 0.159, 3: 0.121, 4: 0....  \n",
            "0           {0: 0.318, 1: 0.184, 2: 0.133, 3: 0.365}  \n",
            "\n",
            "Mejor EXP1: {'exp': 'exp1', 'k': 12, 'silhouette': np.float64(0.581388710104716), 'calinski_harabasz': np.float64(5727.729514628274), 'davies_bouldin': np.float64(0.7865386136640519), 'profile_sharpness': 0.46908086290153017, 'stability_ari': 0.9555077869244603, 'min_size_ok': np.True_, 'sizes': {0: 0.055, 1: 0.235, 2: 0.075, 3: 0.092, 4: 0.04, 5: 0.125, 6: 0.142, 7: 0.045, 8: 0.059, 9: 0.024, 10: 0.056, 11: 0.053}}\n",
            "Entropía geográfica (promedio): 0.831\n",
            "\n",
            "Centroides EXP1:\n",
            "         Servicios_Telecomunicaciones_No  Servicios_Telecomunicaciones_Si  \\\n",
            "cluster                                                                     \n",
            "0                                  0.014                            0.986   \n",
            "1                                  0.009                            0.991   \n",
            "2                                  0.000                            1.000   \n",
            "3                                  0.000                            1.000   \n",
            "4                                  0.017                            0.983   \n",
            "5                                  0.000                            1.000   \n",
            "6                                  0.019                            0.981   \n",
            "7                                  1.000                            0.000   \n",
            "8                                  0.020                            0.980   \n",
            "9                                  0.109                            0.891   \n",
            "10                                 0.070                            0.930   \n",
            "11                                 0.000                            1.000   \n",
            "\n",
            "         Dispositivos_hogar_No  conexion_hogar_si  interrupciones_si  \\\n",
            "cluster                                                                \n",
            "0                        0.000              1.000              0.000   \n",
            "1                        0.000              0.993              1.000   \n",
            "2                        1.000              0.000              0.000   \n",
            "3                        0.000              1.000              0.000   \n",
            "4                        0.000              1.000              0.323   \n",
            "5                        0.000              1.000              0.000   \n",
            "6                        0.000              0.994              1.000   \n",
            "7                        0.644              0.099              0.017   \n",
            "8                        0.000              0.987              1.000   \n",
            "9                        0.560              0.000              0.005   \n",
            "10                       0.571              0.000              0.000   \n",
            "11                       0.000              0.000              0.015   \n",
            "\n",
            "         frec_uso_si  dens_int  \n",
            "cluster                         \n",
            "0                1.0     1.119  \n",
            "1                1.0    -0.835  \n",
            "2                1.0    -0.387  \n",
            "3                1.0    -0.009  \n",
            "4                1.0     2.434  \n",
            "5                1.0    -0.853  \n",
            "6                1.0     0.075  \n",
            "7                1.0    -0.148  \n",
            "8                1.0     1.007  \n",
            "9                1.0     2.673  \n",
            "10               1.0     1.213  \n",
            "11               1.0    -0.395  \n",
            "\n",
            "Tamaños EXP1:\n",
            "            n   pct\n",
            "cluster            \n",
            "0         418   5.5\n",
            "1        1803  23.5\n",
            "2         575   7.5\n",
            "3         704   9.2\n",
            "4         303   4.0\n",
            "5         954  12.5\n",
            "6        1085  14.2\n",
            "7         343   4.5\n",
            "8         455   5.9\n",
            "9         184   2.4\n",
            "10        431   5.6\n",
            "11        407   5.3\n",
            "\n",
            "Top departamentos EXP1:\n",
            "                                        share\n",
            "cluster cluster Nombre Departamento          \n",
            "0       0       NORTE DE SANTANDER   0.248804\n",
            "                VALLE DEL CAUCA      0.098086\n",
            "                ANTIOQUIA            0.095694\n",
            "                CHOCÓ                0.083732\n",
            "                BOLÍVAR              0.071770\n",
            "1       1       BOGOTÁ, D.C.         0.265668\n",
            "                ANTIOQUIA            0.253466\n",
            "                VALLE DEL CAUCA      0.175818\n",
            "                SANTANDER            0.112035\n",
            "                META                 0.052690\n",
            "2       2       ANTIOQUIA            0.186087\n",
            "                ATLÁNTICO            0.165217\n",
            "                VALLE DEL CAUCA      0.125217\n",
            "                BOGOTÁ, D.C.         0.111304\n",
            "                TOLIMA               0.088696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "63270f39",
      "metadata": {
        "id": "63270f39"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- 1) Features para clustering (las que usaste en EXP1) ---\n",
        "FEATURES = [\n",
        "    \"Servicios_Telecomunicaciones_No\",\n",
        "    \"Servicios_Telecomunicaciones_Si\",\n",
        "    \"Dispositivos_hogar_No\",\n",
        "    \"conexion_hogar_si\",\n",
        "    \"interrupciones_si\",\n",
        "    \"frec_uso_si\",\n",
        "    \"dens_int\",\n",
        "]\n",
        "\n",
        "# --- 2) Preparar matriz de entrenamiento ---\n",
        "X = df[FEATURES].copy()\n",
        "scaler = StandardScaler()\n",
        "X[[\"dens_int\"]] = scaler.fit_transform(X[[\"dens_int\"]])  # solo escalar dens_int\n",
        "\n",
        "# --- 3) Entrenar K-means con k=12 (configuración seleccionada) ---\n",
        "kmeans = KMeans(n_clusters=12, n_init=50, random_state=123)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Añadir columna de cluster al dataframe original\n",
        "df_lab = df.copy()\n",
        "df_lab[\"cluster\"] = labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aplicación en dataset ficticio"
      ],
      "metadata": {
        "id": "D0ePTWknCs_w"
      },
      "id": "D0ePTWknCs_w"
    },
    {
      "cell_type": "code",
      "source": [
        "features = X.columns\n",
        "\n",
        "# Seleccionar solo esas columnas en la nueva muestra\n",
        "X_ficticio = df_ficticio[features]\n",
        "\n",
        "# Aplicar predicción de clusters\n",
        "labels_test = kmeans.predict(X_ficticio)\n",
        "\n",
        "# Añadir al df ficticio\n",
        "df_ficticio_lab = df_ficticio.copy()\n",
        "df_ficticio_lab[\"cluster\"] = labels_test"
      ],
      "metadata": {
        "id": "4ULLPA_4qtSm"
      },
      "id": "4ULLPA_4qtSm",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sil_ficticio = silhouette_score(X_ficticio, labels_test)\n",
        "print(\"Silhouette en nueva muestra:\", sil_ficticio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCm_cDL8Cd69",
        "outputId": "372249eb-64aa-4e15-f5f5-6acee7a9c064"
      },
      "id": "dCm_cDL8Cd69",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette en nueva muestra: 0.07785950073257869\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}